import streamlit as st
import pandas as pd
import os
import sys
import importlib
from datetime import datetime, date
import requests
import json
import re

# --- Project Root Setup ---
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# --- Module Imports and Initialization ---
try:
    # These imports assume you have local files with these names.
    # If the app is a single file, you might not need these.
    import config
    importlib.reload(config)
    from data_ingestion import legislative_scraper
    importlib.reload(legislative_scraper)
    from nlp_processing import nlp_processor
    importlib.reload(nlp_processor)
    from database import db_manager
    importlib.reload(db_manager)
    from opportunity_identification import opportunity_identifier
    importlib.reload(opportunity_identifier)

    db_manager_instance = db_manager.DBManager()
    nlp_processor_instance = nlp_processor.NLPProcessor()
    opportunity_identifier_instance = opportunity_identifier.OpportunityIdentifier()

except Exception as e:
    # This error will show if the local files (config.py, etc.) are not found.
    st.error(f"Î£Ï†Î¬Î»Î¼Î± ÎºÎ±Ï„Î¬ Ï„Î· Ï†ÏŒÏÏ„Ï‰ÏƒÎ· Ï„Ï‰Î½ ÎµÎ½Î¿Ï„Î®Ï„Ï‰Î½: {e}")
    st.info("Î’ÎµÎ²Î±Î¹Ï‰Î¸ÎµÎ¯Ï„Îµ ÏŒÏ„Î¹ Ï„Î± Î±ÏÏ‡ÎµÎ¯Î± .py (config, legislative_scraper, Îº.Î»Ï€.) Î²ÏÎ¯ÏƒÎºÎ¿Î½Ï„Î±Î¹ ÏƒÏ„Î¿Î½ ÏƒÏ‰ÏƒÏ„ÏŒ ÎºÎ±Ï„Î¬Î»Î¿Î³Î¿.")
    st.stop()

st.set_page_config(layout="wide", page_title="AI Product Opportunity Identifier")

# --- Core Application Functions ---

def run_pipeline():
    """Scrapes, processes, and stores new opportunity data."""
    st.info("Î•ÎºÏ„ÎµÎ»ÎµÎ¯Ï„Î±Î¹ Î· Î´Î¹Î±Î´Î¹ÎºÎ±ÏƒÎ¯Î± ÏƒÏ…Î»Î»Î¿Î³Î®Ï‚ & Î±Î½Î¬Î»Ï…ÏƒÎ·Ï‚ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½... Î‘Ï…Ï„ÏŒ Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± Î´Î¹Î±ÏÎºÎ­ÏƒÎµÎ¹ Î¼ÎµÏÎ¹ÎºÎ¬ Î»ÎµÏ€Ï„Î¬.")
    
    latest_legislative_news_df = legislative_scraper.get_latest_legislative_news(current_config=config, filter_by_current_date=False)

    processed_df = pd.DataFrame()
    if not latest_legislative_news_df.empty:
        processed_df = nlp_processor_instance.process_dataframe(latest_legislative_news_df)
    
    identified_opportunities_df = pd.DataFrame()
    if not processed_df.empty:
        identified_opportunities_df = opportunity_identifier_instance.identify_and_score_opportunities(processed_df)

    db_manager_instance.connect()
    db_manager_instance.create_table()
    if not identified_opportunities_df.empty:
        db_manager_instance.insert_opportunities(identified_opportunities_df)
    db_manager_instance.close()

    st.success("Î— Î´Î¹Î±Î´Î¹ÎºÎ±ÏƒÎ¯Î± Î¿Î»Î¿ÎºÎ»Î·ÏÏÎ¸Î·ÎºÎµ! Î¤Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î±Î½Î±Î½ÎµÏÎ¸Î·ÎºÎ±Î½.")
    return identified_opportunities_df

def generate_gemini_response(chat_history, api_key):
    """Generates a response from the Gemini API based on chat history."""
    if not api_key:
        return "Î Î±ÏÎ±ÎºÎ±Î»Ï ÎµÎ¹ÏƒÎ±Î³Î¬Î³ÎµÏ„Îµ Ï„Î¿ Gemini API Key ÏƒÎ±Ï‚ ÏƒÏ„Î·Î½ Ï€Î»Î±ÏŠÎ½Î® Î¼Ï€Î¬ÏÎ± Î³Î¹Î± Î½Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÎµÏ„Îµ Ï„Î¿ chatbot."

    api_url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent"
    
    headers = {
        "Content-Type": "application/json"
    }
    
    payload = {
        "contents": chat_history,
        "generationConfig": {
            "temperature": 0.6,
            "maxOutputTokens": 1024
        }
    }

    try:
        response = requests.post(f"{api_url}?key={api_key}", headers=headers, data=json.dumps(payload))
        response.raise_for_status()
        result = response.json()

        if result.get("candidates") and result["candidates"][0].get("content", {}).get("parts"):
            return result["candidates"][0]["content"]["parts"][0]["text"]
        elif result.get("promptFeedback", {}).get("blockReason"):
            return f"Î— Î±Ï€Î¬Î½Ï„Î·ÏƒÎ· Ï„Î¿Ï… Î¼Î¿Î½Ï„Î­Î»Î¿Ï… Î¼Ï€Î»Î¿ÎºÎ±ÏÎ¯ÏƒÏ„Î·ÎºÎµ Î»ÏŒÎ³Ï‰: {result['promptFeedback']['blockReason']}. Î Î±ÏÎ±ÎºÎ±Î»Ï Î´Î¿ÎºÎ¹Î¼Î¬ÏƒÏ„Îµ Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÎ® ÎµÏÏÏ„Î·ÏƒÎ·."
        else:
            st.error(f"Î›Î®Ï†Î¸Î·ÎºÎµ Î¼Î· Î±Î½Î±Î¼ÎµÎ½ÏŒÎ¼ÎµÎ½Î· Î´Î¿Î¼Î® Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·Ï‚ Î±Ï€ÏŒ Ï„Î¿ API: {result}")
            return "Î”ÎµÎ½ Î®Ï„Î±Î½ Î´Ï…Î½Î±Ï„Î® Î· Î»Î®ÏˆÎ· Î­Î³ÎºÏ…ÏÎ·Ï‚ Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·Ï‚ Î±Ï€ÏŒ Ï„Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿."
            
    except requests.exceptions.HTTPError as e:
        if e.response.status_code == 400:
            error_details = e.response.json()
            return f"Î£Ï†Î¬Î»Î¼Î± API (400 - Bad Request): Î¤Î¿ Î±Î¯Ï„Î·Î¼Î± Î®Ï„Î±Î½ ÎµÏƒÏ†Î±Î»Î¼Î­Î½Î¿. Î‘Ï…Ï„ÏŒ Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± Î¿Ï†ÎµÎ¯Î»ÎµÏ„Î±Î¹ ÏƒÎµ Î¼Î· Î­Î³ÎºÏ…ÏÎ¿ ÎºÎ»ÎµÎ¹Î´Î¯ API. Î Î±ÏÎ±ÎºÎ±Î»Ï ÎµÎ»Î­Î³Î¾Ï„Îµ Ï„Î¿ ÎºÎ»ÎµÎ¹Î´Î¯ ÏƒÎ±Ï‚. Î›ÎµÏ€Ï„Î¿Î¼Î­ÏÎµÎ¹ÎµÏ‚: {error_details}"
        elif e.response.status_code == 403:
             return f"Î£Ï†Î¬Î»Î¼Î± API (403 - Forbidden): Î¤Î¿ ÎºÎ»ÎµÎ¹Î´Î¯ API Î´ÎµÎ½ Î­Ï‡ÎµÎ¹ Î´Î¹ÎºÎ±Î¹ÏÎ¼Î±Ï„Î± Î³Î¹Î± Ï„Î¿ Gemini API. Î’ÎµÎ²Î±Î¹Ï‰Î¸ÎµÎ¯Ï„Îµ ÏŒÏ„Î¹ Ï„Î¿ 'Generative Language API' ÎµÎ¯Î½Î±Î¹ ÎµÎ½ÎµÏÎ³Î¿Ï€Î¿Î¹Î·Î¼Î­Î½Î¿ ÏƒÏ„Î¿ Google Cloud project ÏƒÎ±Ï‚."
        return f"Î— ÎºÎ»Î®ÏƒÎ· API Î±Ï€Î­Ï„Ï…Ï‡Îµ Î¼Îµ ÏƒÏ†Î¬Î»Î¼Î± HTTP: {e}. Î•Î»Î­Î³Î¾Ï„Îµ Ï„Î¿ ÎºÎ»ÎµÎ¹Î´Î¯ API ÎºÎ±Î¹ Ï„Î· ÏƒÏÎ½Î´ÎµÏƒÎ® ÏƒÎ±Ï‚ ÏƒÏ„Î¿ Î´Î¹Î±Î´Î¯ÎºÏ„Ï…Î¿."
    except Exception as e:
        return f"Î ÏÎ¿Î­ÎºÏ…ÏˆÎµ Î­Î½Î± Î¼Î· Î±Î½Î±Î¼ÎµÎ½ÏŒÎ¼ÎµÎ½Î¿ ÏƒÏ†Î¬Î»Î¼Î± ÎºÎ±Ï„Î¬ Ï„Î·Î½ ÎºÎ»Î®ÏƒÎ· API: {e}"

# --- Main Streamlit UI ---

st.header("AI Product Opportunity Identifier ğŸ’¡")
st.markdown("Î‘Î½Î±ÎºÎ±Î»ÏÏ€Ï„Î¿Î½Ï„Î±Ï‚ Î½Î­ÎµÏ‚ ÎµÏ…ÎºÎ±Î¹ÏÎ¯ÎµÏ‚ ÏƒÏ„Î¿Ï…Ï‚ Ï†Î¿ÏÎ¿Î»Î¿Î³Î¹ÎºÎ¿ÏÏ‚ ÎºÎ±Î¹ Î¿Î¹ÎºÎ¿Î½Î¿Î¼Î¹ÎºÎ¿ÏÏ‚ Ï„Î¿Î¼ÎµÎ¯Ï‚.")

# --- Sidebar ---
with st.sidebar:
    st.title("Î Î¯Î½Î±ÎºÎ±Ï‚ Î•Î»Î­Î³Ï‡Î¿Ï…")

    gemini_api_key = st.secrets.get("GEMINI_API_KEY")
    if not gemini_api_key:
        gemini_api_key = st.text_input("Î•Î¹ÏƒÎ±Î³Î¬Î³ÎµÏ„Îµ Ï„Î¿ Gemini API Key:", type="password", help="ÎœÏ€Î¿ÏÎµÎ¯Ï„Îµ Î½Î± Î²ÏÎµÎ¯Ï„Îµ Ï„Î¿ ÎºÎ»ÎµÎ¹Î´Î¯ ÏƒÎ±Ï‚ ÏƒÏ„Î¿ Google AI Studio.")
        if not gemini_api_key:
            st.warning("Î Î±ÏÎ±ÎºÎ±Î»Ï ÎµÎ¹ÏƒÎ±Î³Î¬Î³ÎµÏ„Îµ Ï„Î¿ ÎºÎ»ÎµÎ¹Î´Î¯ API Î³Î¹Î± Î½Î± ÎµÎ½ÎµÏÎ³Î¿Ï€Î¿Î¹Î®ÏƒÎµÏ„Îµ Ï„Î¿ chatbot.")
        else:
            st.success("Î¤Î¿ ÎºÎ»ÎµÎ¹Î´Î¯ API Î´ÏŒÎ¸Î·ÎºÎµ.")
    else:
        st.success("Î¤Î¿ ÎºÎ»ÎµÎ¹Î´Î¯ API Ï†Î¿ÏÏ„ÏÎ¸Î·ÎºÎµ Î¼Îµ ÎµÏ€Î¹Ï„Ï…Ï‡Î¯Î±.")

    if st.button("Î‘Î½Î±Î½Î­Ï‰ÏƒÎ· Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ & Î•Î½Ï„Î¿Ï€Î¹ÏƒÎ¼ÏŒÏ‚ Î•Ï…ÎºÎ±Î¹ÏÎ¹ÏÎ½", help="Î•ÎºÏ„ÎµÎ»Î­ÏƒÏ„Îµ Î¾Î±Î½Î¬ ÏŒÎ»Î· Ï„Î· Î´Î¹Î±Î´Î¹ÎºÎ±ÏƒÎ¯Î± Î³Î¹Î± Î½Î± Î²ÏÎµÎ¯Ï„Îµ Î½Î­ÎµÏ‚ ÎµÏ…ÎºÎ±Î¹ÏÎ¯ÎµÏ‚."):
        st.session_state['refresh_data'] = True

    st.markdown("---")
    st.subheader("Î£Ï‡ÎµÏ„Î¹ÎºÎ¬ Î¼Îµ Ï„Î·Î½ Î•Ï†Î±ÏÎ¼Î¿Î³Î®")
    st.info(
        "Î‘Ï…Ï„Î® Î· ÎµÏ†Î±ÏÎ¼Î¿Î³Î® ÏƒÏ…Î»Î»Î­Î³ÎµÎ¹ Î±Ï…Ï„ÏŒÎ¼Î±Ï„Î± Ï†Î¿ÏÎ¿Î»Î¿Î³Î¹ÎºÎ­Ï‚ ÎºÎ±Î¹ Î¿Î¹ÎºÎ¿Î½Î¿Î¼Î¹ÎºÎ­Ï‚ ÎµÎ¹Î´Î®ÏƒÎµÎ¹Ï‚, Ï„Î¹Ï‚ ÎµÏ€ÎµÎ¾ÎµÏÎ³Î¬Î¶ÎµÏ„Î±Î¹ Î¼Îµ NLP, "
        "ÎµÎ½Ï„Î¿Ï€Î¯Î¶ÎµÎ¹ Ï€Î¹Î¸Î±Î½Î­Ï‚ ÏƒÏ…Î¼Î²Î¿Ï…Î»ÎµÏ…Ï„Î¹ÎºÎ­Ï‚ ÎµÏ…ÎºÎ±Î¹ÏÎ¯ÎµÏ‚ ÎºÎ±Î¹ Ï„Î¹Ï‚ ÎµÎ¼Ï†Î±Î½Î¯Î¶ÎµÎ¹ ÏƒÎµ Î­Î½Î±Î½ Î´Î¹Î±Î´ÏÎ±ÏƒÏ„Î¹ÎºÏŒ Ï€Î¯Î½Î±ÎºÎ±."
    )
    if st.button("Î’Î¿Î®Î¸ÎµÎ¹Î± Î±Ï€ÏŒ Ï„Î¿ Chatbot ğŸ’¬", help="Î‘Î½Î¿Î¯Î¾Ï„Îµ Ï„Î¿ chat Î³Î¹Î± Î½Î± ÎºÎ¬Î½ÎµÏ„Îµ ÎµÏÏ‰Ï„Î®ÏƒÎµÎ¹Ï‚."):
        st.session_state['show_chatbot'] = not st.session_state.get('show_chatbot', False)
    st.markdown("---")

# --- Session State Initialization ---
if 'last_identified_df' not in st.session_state:
    st.session_state['last_identified_df'] = pd.DataFrame()
if 'chat_history' not in st.session_state:
    st.session_state['chat_history'] = []
if 'show_chatbot' not in st.session_state:
    st.session_state['show_chatbot'] = False
if 'refresh_data' not in st.session_state:
    st.session_state['refresh_data'] = False

# --- Data Loading Logic ---
if st.session_state.refresh_data:
    identified_opportunities_df = run_pipeline()
    st.session_state['last_identified_df'] = identified_opportunities_df
    st.session_state.chat_history = []
    st.session_state.refresh_data = False
    st.rerun()
else:
    if st.session_state.last_identified_df.empty:
        with st.spinner("Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Î±ÏÏ‡Î¹ÎºÏÎ½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Î±Ï€ÏŒ Ï„Î· Î²Î¬ÏƒÎ·..."):
            db_manager_instance.connect()
            all_stored_data_df = db_manager_instance.fetch_all_opportunities()
            db_manager_instance.close()

            if not all_stored_data_df.empty:
                identified_opportunities_df = all_stored_data_df[all_stored_data_df['opportunity_score'] > 0].copy()
                identified_opportunities_df = identified_opportunities_df.sort_values(by='opportunity_score', ascending=False)
            else:
                identified_opportunities_df = pd.DataFrame()
            st.session_state['last_identified_df'] = identified_opportunities_df
    else:
        identified_opportunities_df = st.session_state['last_identified_df']

# --- Display Identified Opportunities ---
st.subheader("Î•Ï€Î¹ÏƒÎºÏŒÏ€Î·ÏƒÎ· Î•Î½Ï„Î¿Ï€Î¹ÏƒÎ¼Î­Î½Ï‰Î½ Î•Ï…ÎºÎ±Î¹ÏÎ¹ÏÎ½")

if identified_opportunities_df.empty:
    st.warning("Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ ÎµÏ…ÎºÎ±Î¹ÏÎ¯ÎµÏ‚. Î Î±Ï„Î®ÏƒÏ„Îµ 'Î‘Î½Î±Î½Î­Ï‰ÏƒÎ· Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½' Î³Î¹Î± Î½Î± Î¾ÎµÎºÎ¹Î½Î®ÏƒÎµÏ„Îµ.")
else:
    total_opportunities = len(identified_opportunities_df)
    st.info(f"Î•Î¼Ï†Î±Î½Î¯Î¶Î¿Î½Ï„Î±Î¹ {total_opportunities} ÎµÎ½Ï„Î¿Ï€Î¹ÏƒÎ¼Î­Î½ÎµÏ‚ ÎµÏ…ÎºÎ±Î¹ÏÎ¯ÎµÏ‚.")
    st.markdown("---")
    st.subheader("Î¦Î¯Î»Ï„ÏÎ± & Î‘Î½Î±Î¶Î®Ï„Î·ÏƒÎ· Î‘Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½")
    
    search_query = st.text_input("Î‘Î½Î±Î¶Î®Ï„Î·ÏƒÎ· Î¼Îµ Î¤Î¯Ï„Î»Î¿ Î® Î›Î­Î¾ÎµÎ¹Ï‚-ÎšÎ»ÎµÎ¹Î´Î¹Î¬:", "")
    col_filter1, col_filter2 = st.columns(2)
    with col_filter1:
        unique_sources = ["ÎŒÎ»ÎµÏ‚"] + list(identified_opportunities_df['source'].unique())
        selected_source = st.selectbox("Î¦Î¯Î»Ï„ÏÎ¿ Î±Î½Î¬ Î Î·Î³Î®:", unique_sources)
    with col_filter2:
        unique_types = ["ÎŒÎ»Î¿Î¹"] + list(identified_opportunities_df['opportunity_type'].dropna().unique())
        selected_type = st.selectbox("Î¦Î¯Î»Ï„ÏÎ¿ Î±Î½Î¬ Î¤ÏÏ€Î¿ Î•Ï…ÎºÎ±Î¹ÏÎ¯Î±Ï‚:", unique_types)

    filtered_df = identified_opportunities_df.copy()
    if search_query:
        filtered_df = filtered_df[
            filtered_df['title'].str.contains(search_query, case=False, na=False) |
            filtered_df['keywords'].str.contains(search_query, case=False, na=False)
        ]
    if selected_source != "ÎŒÎ»ÎµÏ‚":
        filtered_df = filtered_df[filtered_df['source'] == selected_source]
    if selected_type != "ÎŒÎ»Î¿Î¹":
        filtered_df = filtered_df[filtered_df['opportunity_type'] == selected_type]

    if filtered_df.empty:
        st.info("Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ ÎµÏ…ÎºÎ±Î¹ÏÎ¯ÎµÏ‚ Ï€Î¿Ï… Î½Î± Ï„Î±Î¹ÏÎ¹Î¬Î¶Î¿Ï…Î½ Î¼Îµ Ï„Î± ÎµÏ€Î¹Î»ÎµÎ³Î¼Î­Î½Î± Ï†Î¯Î»Ï„ÏÎ±.")
    else:
        display_cols = ['title', 'date', 'source', 'opportunity_score', 'opportunity_type', 'url', 'keywords', 'main_topic']
        st.dataframe(
            filtered_df[display_cols],
            use_container_width=True,
            hide_index=True,
            column_config={
                "url": st.column_config.LinkColumn("URL", display_text="Î£ÏÎ½Î´ÎµÏƒÎ¼Î¿Ï‚"),
                "date": st.column_config.DateColumn("Î—Î¼ÎµÏÎ¿Î¼Î·Î½Î¯Î±", format="DD/MM/YYYY"),
                "opportunity_score": st.column_config.NumberColumn("Î’Î±Î¸Î¼Î¿Î»Î¿Î³Î¯Î±", help="Î’Î±Î¸Î¼ÏŒÏ‚ Î£Î·Î¼Î±Î½Ï„Î¹ÎºÏŒÏ„Î·Ï„Î±Ï‚ (Ï…ÏˆÎ·Î»ÏŒÏ„ÎµÏÎ¿Ï‚ = ÎºÎ±Î»ÏÏ„ÎµÏÎ¿Ï‚)", format="%.1f"),
                "opportunity_type": "Î¤ÏÏ€Î¿Ï‚",
                "title": st.column_config.TextColumn("Î¤Î¯Ï„Î»Î¿Ï‚", width="large"),
                "source": "Î Î·Î³Î®",
                "keywords": "Î›Î­Î¾ÎµÎ¹Ï‚-ÎšÎ»ÎµÎ¹Î´Î¹Î¬",
                "main_topic": "ÎšÏÏÎ¹Î¿ Î˜Î­Î¼Î±"
            }
        )
        csv_data = filtered_df.to_csv(index=False).encode('utf-8')
        st.download_button(
            label="Î›Î®ÏˆÎ· Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Ï‰Ï‚ CSV",
            data=csv_data,
            file_name="identified_opportunities.csv",
            mime="text/csv"
        )

st.markdown("---")

# --- Chatbot Interface ---
if st.session_state.get('show_chatbot', False):
    st.subheader("Î’Î¿Î·Î¸ÏŒÏ‚ Chatbot ğŸ’¬")
    st.markdown("Î¡Ï‰Ï„Î®ÏƒÏ„Îµ Î¼Îµ Î³Î¹Î± Ï„Î¹Ï‚ ÎµÏ…ÎºÎ±Î¹ÏÎ¯ÎµÏ‚ Ï€Î¿Ï… ÎµÎ½Ï„Î¿Ï€Î¯ÏƒÏ„Î·ÎºÎ±Î½ Î® Î³Î¹Î± Î³ÎµÎ½Î¹ÎºÎ¬ Ï†Î¿ÏÎ¿Î»Î¿Î³Î¹ÎºÎ¬/Î¿Î¹ÎºÎ¿Î½Î¿Î¼Î¹ÎºÎ¬ Î¸Î­Î¼Î±Ï„Î±!")

    initial_greeting = "Okay, I understand. I'm ready to assist with your questions about Greek tax and economic opportunities."
    
    if not st.session_state.chat_history:
        system_prompt = (
            "You are an AI assistant specialized in Greek tax and economic opportunities. "
            "Your goal is to provide concise and helpful information based on the provided context. "
            "If the question is about specific opportunities, refer to the provided context. "
            "If the context does not contain the answer, state that you cannot find it in the provided information. "
            "Format your answers clearly using markdown where appropriate."
        )
        st.session_state.chat_history.append({"role": "user", "parts": [{"text": system_prompt}]})
        st.session_state.chat_history.append({"role": "model", "parts": [{"text": initial_greeting}]})

    # Display chat history, but hide the initial system prompt and canned model response
    for message in st.session_state.chat_history:
        is_system_prompt = message["role"] == "user" and "specialized in Greek tax" in message["parts"][0]["text"]
        is_initial_greeting = message["role"] == "model" and message["parts"][0]["text"] == initial_greeting
        
        if not is_system_prompt and not is_initial_greeting:
            with st.chat_message(message["role"]):
                st.markdown(message["parts"][0]["text"])
    
    st.markdown("---")
    st.markdown("**ÎšÎ¬Î½Ï„Îµ ÎºÎ»Î¹Îº ÏƒÎµ Î¼Î¹Î± ÎµÏÏÏ„Î·ÏƒÎ· Î³Î¹Î± Î½Î± Î¾ÎµÎºÎ¹Î½Î®ÏƒÎµÏ„Îµ:**")
    
    suggested_questions = [
        "Î Î¿Î¹ÎµÏ‚ ÎµÎ¯Î½Î±Î¹ Î¿Î¹ Ï„ÎµÎ»ÎµÏ…Ï„Î±Î¯ÎµÏ‚ Î±Î»Î»Î±Î³Î­Ï‚ ÏƒÏ„Î· Ï†Î¿ÏÎ¿Î»Î¿Î³Î¹ÎºÎ® Î½Î¿Î¼Î¿Î¸ÎµÏƒÎ¯Î±;",
        "Î£Ï…Î½Î¿ÏˆÎ¯ÏƒÏ„Îµ Ï„Î¹Ï‚ ÏƒÎ·Î¼Î±Î½Ï„Î¹ÎºÏŒÏ„ÎµÏÎµÏ‚ ÎµÏ…ÎºÎ±Î¹ÏÎ¯ÎµÏ‚.",
        "Î ÎµÎ¯Ï„Îµ Î¼Î¿Ï… Î³Î¹Î± ÎµÏ…ÎºÎ±Î¹ÏÎ¯ÎµÏ‚ Ï€Î¿Ï… ÏƒÏ‡ÎµÏ„Î¯Î¶Î¿Î½Ï„Î±Î¹ Î¼Îµ ÎºÎ¯Î½Î·Ï„ÏÎ±."
    ]
    
    query_to_send = None
    cols = st.columns(len(suggested_questions))
    for i, question in enumerate(suggested_questions):
        if cols[i].button(question, key=f"suggested_q_{i}"):
            query_to_send = question
    
    if user_input := st.chat_input("Î— ÎµÏÏÏ„Î·ÏƒÎ® ÏƒÎ±Ï‚:"):
        query_to_send = user_input

    if query_to_send:
        st.session_state.chat_history.append({"role": "user", "parts": [{"text": query_to_send}]})
        
        with st.chat_message("user"):
            st.markdown(query_to_send)

        with st.chat_message("assistant"):
            with st.spinner("Î£ÎºÎ­Ï†Ï„Î¿Î¼Î±Î¹..."):
                context_df = st.session_state.get('last_identified_df', pd.DataFrame())
                context_str = ""
                if not context_df.empty:
                    context_str = "Context from the database:\n"
                    for _, row in context_df.head(15).iterrows():
                        context_str += (
                            f"- Title: {row.get('title', 'N/A')}\n"
                            f"  Summary: {row.get('summary', 'N/A')}\n"
                            f"  Type: {row.get('opportunity_type', 'N/A')}\n"
                            f"  Score: {row.get('opportunity_score', 0):.1f}\n---\n"
                        )
                
                api_chat_history = st.session_state.chat_history.copy()
                api_chat_history.insert(-1, {"role": "user", "parts": [{"text": f"Use this context to answer the user's last question:\n{context_str}"}]})
                api_chat_history.insert(-1, {"role": "model", "parts": [{"text": "Okay, I will use the provided context."}]})

                response_text = generate_gemini_response(api_chat_history, gemini_api_key)
                st.markdown(response_text)
        
        st.session_state.chat_history.append({"role": "model", "parts": [{"text": response_text}]})
        st.rerun()